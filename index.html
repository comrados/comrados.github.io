<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Georgii Mikriukov</title>
    <!-- Bootswatch Cosmo Theme (Bootstrap 5) -->
    <link href="https://cdn.jsdelivr.net/npm/bootswatch@5.3.0/dist/cosmo/bootstrap.min.css" rel="stylesheet" />
    <!-- Font Awesome Icons -->
    <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet" />
    <!-- Animate.css for animations -->
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/4.1.1/animate.min.css" />
    <!-- Custom CSS -->
    <link rel="stylesheet" href="assets/css/styles.css" />
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;600;700&display=swap" rel="stylesheet">
</head>



<body>
    <!-- Navigation Bar -->
    <nav class="navbar navbar-expand-lg navbar-dark bg-primary fixed-top">
        <div class="container">
            <a class="navbar-brand" href="#"><strong>Georgii Mikriukov</strong></a>
            <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarNav">
                <span class="navbar-toggler-icon"></span>
            </button>
            <div class="collapse navbar-collapse" id="navbarNav">
                <ul class="navbar-nav ms-auto">
                    <li class="nav-item"><a class="nav-link" href="#about">About</a></li>
                    <li class="nav-item"><a class="nav-link" href="#projects">Projects</a></li>
                    <li class="nav-item"><a class="nav-link" href="#publications">Publications</a></li>
                    <li class="nav-item"><a class="nav-link" href="#patents">Patents</a></li>
                    <li class="nav-item"><a class="nav-link" href="#contact">Contact</a></li>
                </ul>
            </div>
        </div>
    </nav>



    <!-- Header / Hero Section -->
    <header class="hero-section text-white" style="padding-top:100px; padding-bottom:100px;">
        <div class="container">
            <div class="row align-items-center">
                <div class="col-md-8">
                    <h1 class="display-4"><strong>Georgii Mikriukov</strong></h1>
                    <p class="lead">
                        <strong>AI Research Scientist &amp; PhD Student</strong><br>
                        <small>Berlin, Germany</small>
                    </p>
                    <div class="social-links mt-3">
                        <a href="mailto:mikriukov.georgii@gmail.com" class="text-white me-3">
                            <i class="fas fa-envelope"></i>
                        </a>
                        <a href="https://github.com/comrados" class="text-white me-3" target="_blank">
                            <i class="fab fa-github"></i>
                        </a>
                        <a href="https://linkedin.com/in/georgii-mikriukov" class="text-white me-3" target="_blank">
                            <i class="fab fa-linkedin"></i>
                        </a>
                        <a href="https://scholar.google.com/citations?user=hCu7avEAAAAJ&hl=en" class="text-white me-3"
                            target="_blank">
                            <i class="fas fa-graduation-cap"></i>
                        </a>
                        <!-- ResearchGate Icon -->
                        <a href="https://www.researchgate.net/profile/Georgii-Mikriukov-2" class="text-white me-3"
                            target="_blank">
                            <i class="fab fa-researchgate"></i>
                        </a>
                    </div>
                    <a href="assets/cv/CV_Georgii_Mikriukov.pdf" download class="btn btn-cv btn-lg mt-3">
                        <i class="fas fa-download me-2"></i>Download CV
                    </a>
                </div>
                <div class="col-md-4 text-center">
                    <img src="assets/images/photo.jpg" alt="Georgii Mikriukov" class="img-fluid rounded-circle shadow"
                        style="max-width: 200px;">
                </div>
            </div>
        </div>
    </header>



    <!-- About Me Section -->
    <section id="about" class="section py-5">
        <div class="container">
            <h2 class="section-title mb-4">About Me</h2>
            <p>
                <strong>AI Research Scientist and PhD Student </strong> with <strong>4+ years</strong> of experience in
                <strong>machine
                    learning and deep learning</strong>, specializing in <strong>computer vision, NLP, and eXplainable
                    AI</strong>. Extensive hands-on experience in developing and evaluating <strong>CNNs, ViTs, LLMs,
                    and multi-modal architectures</strong>. Proven expertise in <strong>model verification</strong> and
                <strong>large-scale multi-modal information retrieval</strong>, particularly in <strong>remote sensing
                    and autonomous driving</strong>.
            </p>
        </div>
    </section>



    <!-- Projects Section -->
    <section id="projects" class="section py-5">
        <div class="container">
            <h2 class="section-title mb-4">Projects</h2>

            <!-- Scrollable Project Tiles -->
            <div class="projects-container">
                <div class="project-tile" data-project="1">
                    <h5><strong>Local Concept Embeddings</strong></h5>
                </div>
                <div class="project-tile" data-project="2">
                    <h5><strong>Deep Unsupervised Contrastive Hashing</strong></h5>
                </div>
                <div class="project-tile" data-project="3">
                    <h5><strong>Cross-Modal Hashing with Noise Robustness</strong></h5>
                </div>
            </div>

            <!-- Project Details (Initially Hidden) -->
            <div class="project-details-container">
                <!-- Project 1 -->
                <div class="project-details" data-project="1">
                    <h4><strong>Local Concept Embeddings</strong></h4>
                    <p>
                        <strong>LoCEs (Local Concept Embeddings)</strong> analyze how <strong>DNNs represent visual concepts</strong> (e.g., object) within complex scenes by generating 
                        <strong>sample-specific embeddings</strong>. Unlike traditional methods, LoCEs capture both the <strong>foreground concept</strong> and its 
                        <strong>context</strong> (background) within a single representation.
                    </p>                    
                    <p>
                        As <strong>concept-vs-context embeddings</strong>, LoCEs provide insights into not only the concept itself but also the environments 
                        in which it appears. Analyzing these embeddings reveals <strong>concept distributions, relationships, and variations</strong>, 
                        offering a deeper understanding of how neural networks perceive objects in <strong>complex scenes</strong>.
                    </p>

                    <p>For example, <strong>LoCEs</strong> can be used for:</p>
                    <ul>
                        <li><strong>Concept Understanding</strong> – Analysis of how a model distinguishes between
                            different objects and their contexts in complex scenes.</li>
                        <li><strong>Sub-Concept Discovery</strong> – Identification of finer details within a concept, such as <strong>near
                            car</strong> vs. <strong>distant car</strong>.
                        </li>
                        <li><strong>Concept Confusion Detection</strong> – Identification of cases where a model
                            struggles to differentiate between similar objects, like <strong>bus</strong> and <strong>truck</strong>.</li>
                        <li><strong>Outlier Detection</strong> – Detection of unusual or mislabeled data points.</li>
                        <li><strong>Information Retrieval</strong> – Retrieval of images based on concept similarity in
                            feature space.</li>
                        <li><strong>Comparison of Feature Spaces in Networks</strong> – Evaluating how different DNN
                            architectures or training methods affect concept representation, enabling insights into
                            generalization, robustness, and feature alignment across models.</li>
                    </ul>
                    <div class="project-image-container text-center">
                        <figure>
                            <img src="assets/images/loce_example.png" alt="LoCEs Example"
                                class="img-fluid rounded shadow project-image-preview"
                                onclick="openModal('assets/images/loce_example.png')">
                            <figcaption class="mt-2 text-muted">LoCE optimization and generalization.</figcaption>
                        </figure>
                    </div>

                    <div class="project-image-container text-center">
                        <img src="assets/images/loce_separation.png" alt="LoCEs Concept Separation"
                            class="img-fluid rounded shadow project-image-preview"
                            onclick="openModal('assets/images/loce_separation.png')">
                        <figcaption class="mt-2 text-muted">Concept confusion in different models identified with LoCEs.
                        </figcaption>
                    </div>
                    <div class="text-center mt-3">
                        <a href="https://github.com/continental/local-concept-embeddings" target="_blank"
                            class="btn btn-github btn-lg">
                            <i class="fab fa-github me-2"></i> View on GitHub
                        </a>
                    </div>
                </div>

                <!-- Project 2 -->
                <div class="project-details" data-project="2">
                    <h4><strong>Deep Unsupervised Contrastive Hashing</strong></h4>
                    <p>
                        <strong>DUCH</strong> (Deep Unsupervised Contrastive Hashing) is an <strong>unsupervised
                            cross-modal retrieval method</strong> designed for efficient search and retrieval of
                        <strong>semantically related images and text</strong> in large-scale datasets.
                        Unlike traditional supervised retrieval methods that rely on extensive labeled data,
                        <strong>DUCH</strong> learns <strong>discriminative feature representations</strong> in an
                        unsupervised manner, making it scalable and adaptable for various multi-modal applications.
                    </p>

                    <h5>Key Features:</h5>
                    <ul>
                        <li><strong>Cross-Modal Hashing:</strong> Generates <strong>compact binary hash codes</strong>
                            for <strong>fast and memory-efficient</strong> retrieval across different modalities (e.g.,
                            image-to-text, text-to-image).</li>
                        <li><strong>Unsupervised Contrastive Learning:</strong> Leverages <strong>contrastive
                                objectives</strong> to preserve semantic relationships, enabling effective retrieval
                            without labeled training data.</li>
                        <li><strong>Adversarial Learning:</strong> Enforces alignment between <strong>image and text
                                representations</strong> using an adversarial loss function, improving retrieval
                            consistency.</li>
                        <li><strong>Multi-Objective Optimization:</strong> Combines multiple loss functions to enhance
                            retrieval performance.
                        </li>
                    </ul>

                    <h5>Applications:</h5>
                    <ul>
                        <li><strong>Multi-Modal Search:</strong> Enables <strong>image-to-text and text-to-image
                                retrieval</strong> in diverse domains, including visual recognition, media analysis, and
                            scientific research.</li>
                        <li><strong>Scalable Indexing:</strong> Compact hash codes enable <strong>efficient large-scale
                                data retrieval</strong> with minimal storage and computational cost.</li>
                        <li><strong>Semantic Representation Learning:</strong> Captures complex relationships between
                            different modalities, aiding in knowledge discovery and AI-driven content understanding.
                        </li>
                    </ul>


                    <div class="project-image-container text-center">
                        <img src="assets/images/duch_example.png" alt="DUCH Architecture"
                            class="img-fluid rounded shadow project-image-preview"
                            onclick="openModal('assets/images/duch_example.png')">
                        <figcaption class="mt-2 text-muted">DUCH Architecture.</figcaption>
                    </div>

                    <div class="text-center mt-3">
                        <a href="https://git.tu-berlin.de/rsim/duch" target="_blank" class="btn btn-github btn-lg">
                            <i class="fab fa-gitlab me-2"></i> View on GitLab
                        </a>
                    </div>
                </div>

                <!-- Project 3 -->
                <div class="project-details" data-project="3">
                    <h4><strong>Cross-Modal Hashing with Noise Robustness</strong></h4>
                    <p>
                        <strong>CHNR</strong> (Cross-Modal Hashing with Noise Robustness) is an
                        <strong>unsupervised</strong> technique designed for
                        <strong>retrieving images based on textual descriptions</strong>, even in the presence of noisy
                        image-text correspondences.
                        It extends <a href="#" class="duch-link"><strong>DUCH (Deep Unsupervised Contrastive Hashing)</strong></a> by introducing a <strong>noise detection module</strong> that mitigates errors in training data.
                    </p>

                    <ul>
                        <li><strong>Unsupervised Learning</strong> – Learns robust cross-modal representations without
                            labeled data, making it adaptable to large-scale datasets.</li>
                        <li><strong>Robust Feature Extraction</strong> – Extracts deep representations of both images
                            and texts, ensuring effective cross-modal learning.</li>
                        <li><strong>Noise Detection Module</strong> – Identifies and reduces the impact of incorrectly
                            paired image-text data.</li>
                        <li><strong>Efficient Hashing Mechanism</strong> – Generates binary hash codes for scalable
                            retrieval with minimal storage requirements.</li>
                    </ul>


                    <div class="project-image-container text-center">
                        <img src="assets/images/chnr_example.png" alt="CHNR Architecture"
                            class="img-fluid rounded shadow project-image-preview"
                            onclick="openModal('assets/images/chnr_example.png')">
                        <figcaption class="mt-2 text-muted">CHNR Architecture.</figcaption>
                    </div>

                    <div class="text-center mt-3">
                        <a href="https://git.tu-berlin.de/rsim/chnr" target="_blank" class="btn btn-github btn-lg">
                            <i class="fab fa-gitlab me-2"></i> View on GitLab
                        </a>
                    </div>
                </div>

            </div>

            <!-- Full-Screen Image Modal -->
            <div id="imageModal" class="modal" onclick="closeModal()">
                <span class="close">&times;</span>
                <img class="modal-content" id="fullImage">
            </div>
        </div>
    </section>


    <!-- Publications Section -->
    <section id="publications" class="section py-5">
        <div class="container">
            <h2 class="section-title mb-4">Publications</h2>

            <!-- Publication Card 1 -->
            <div class="card publication-card mb-3">
                <div class="card-body">
                    <h5 class="card-title">
                        <a href="https://ieeexplore.ieee.org/document/9746251" target="_blank">Unsupervised Contrastive
                            Hashing for Cross-Modal Retrieval in Remote
                            Sensing</a>
                    </h5>
                    <p class="card-text">
                        IEEE International Conference on Acoustics, Speech and Signal Processing, ICASSP 2022
                    </p>
                    <p class="card-text-small">
                        <strong>Georgii Mikriukov</strong>, Mahdyar Ravanbakhsh, Begüm Demir.
                    </p>
                </div>
            </div>

            <!-- Publication Card 2 -->
            <div class="card publication-card mb-3">
                <div class="card-body">
                    <h5 class="card-title">
                        <a href="https://ieeexplore.ieee.org/document/9897500" target="_blank">An Unsupervised
                            Cross-Modal Hashing Method Robust to Noisy Training
                            Image-Text Correspondences</a>
                    </h5>
                    <p class="card-text">
                        IEEE International Conference on Image Processing, ICIP 2022
                    </p>
                    <p class="card-text-small">
                        <strong>Georgii Mikriukov</strong>, Mahdyar Ravanbakhsh, Begüm Demir.
                    </p>
                </div>
            </div>

            <!-- Publication Card 3 -->
            <div class="card publication-card mb-3">
                <div class="card-body">
                    <h5 class="card-title">
                        <a href="https://link.springer.com/chapter/10.1007/978-3-031-44067-0_26" target="_blank">
                            Evaluating the Stability of Semantic Concept Representations in CNNs
                        </a>
                    </h5>
                    <p class="card-text">
                        Explainable Artificial Intelligence, xAI 2023.
                    </p>
                    <p class="card-text-small">
                        <strong>Georgii Mikriukov</strong>, Gesina Schwalbe, Christian Hellert, Korinna Bade.
                    </p>
                    <p class="card-text-small award-highlight">
                        Best industry paper award:
                        <a href="https://xaiworldconference.com/2023/awards/" target="_blank">
                            Awards – The World Conference on eXplainable Artificial Intelligence
                        </a>
                    </p>
                </div>
            </div>

            <!-- Publication Card 4 -->
            <div class="card publication-card mb-3">
                <div class="card-body">
                    <h5 class="card-title">
                        <a href="https://link.springer.com/chapter/10.1007/978-3-031-74630-7_1"
                            target="_blank">Revealing Similar Semantics Inside CNNs: An Interpretable Concept-Based
                            Comparison of Feature Spaces</a>
                    </h5>
                    <p class="card-text">
                        Machine Learning and Principles and Practice of Knowledge Discovery in Databases, ECML PKDD 2023
                    </p>
                    <p class="card-text-small">
                        <strong>Georgii Mikriukov</strong>, Gesina Schwalbe, Christian Hellert, Korinna Bade.
                    </p>
                </div>
            </div>

            <!-- Publication Card 5 -->
            <div class="card publication-card mb-3">
                <div class="card-body">
                    <h5 class="card-title">
                        <a href="https://link.springer.com/chapter/10.1007/978-3-031-63787-2_6"
                            target="_blank">Unveiling the Anatomy of Adversarial Attacks: Concept-Based XAI Dissection
                            of CNNs</a>
                    </h5>
                    <p class="card-text">
                        Explainable Artificial Intelligence, xAI 2024.
                    </p>
                    <p class="card-text-small">
                        <strong>Georgii Mikriukov</strong>, Gesina Schwalbe, Franz Motzkus, Korinna Bade.
                    </p>
                </div>
            </div>

            <!-- Publication Card 6 -->
            <div class="card publication-card mb-3">
                <div class="card-body">
                    <h5 class="card-title">
                        <a href="https://link.springer.com/chapter/10.1007/978-3-031-63787-2_8" target="_blank">Locally
                            Testing Model Detections for Semantic Global Concepts</a>
                    </h5>
                    <p class="card-text">
                        Explainable Artificial Intelligence, xAI 2024.
                    </p>
                    <p class="card-text-small">
                        Franz Motzkus, <strong>Georgii Mikriukov</strong>, Christian Hellert, Ute Schmid
                    </p>
                </div>
            </div>

            <!-- Publication Card 7 -->
            <div class="card publication-card mb-3">
                <div class="card-body">
                    <h5 class="card-title">
                        <a href="https://excv-workshop.github.io/publication/concept-based-explanations-in-computer-vision-where-are-we-and-where-could-we-go/paper.pdf"
                            target="_blank">Concept-Based Explanations in Computer Vision: Where Are We
                            Going?</a>
                    </h5>
                    <p class="card-text">
                        Explainable Computer Vision @ European Conference on Computer Vision, ECCV 2024.
                    </p>
                    <p class="card-text-small">
                        Jae Hee Lee, <strong>Georgii Mikriukov</strong>, Gesina Schwalbe, Stefan Wermter, Diedrich
                        Wolter
                    </p>
                </div>
            </div>

            <!-- Publication Card 8 -->
            <div class="card publication-card mb-3">
                <div class="card-body">
                    <h5 class="card-title">
                        <a href="https://arxiv.org/abs/2311.14435v2" target="_blank">Local Concept Embeddings for
                            Analysis of Concept Distributions in
                            Vision DNN Feature Spaces</a>
                    </h5>
                    <p class="card-text">
                        International Journal of Computer Vision, IJCV (under review since 21.10.24, revision submitted
                        20.01.25).
                    </p>
                    <p class="card-text-small">
                        <strong>Georgii Mikriukov</strong>, Gesina Schwalbe, Korinna Bade.
                    </p>
                </div>
            </div>

            <!-- Add more publication cards as needed -->

        </div>
    </section>




    <!-- Patents Section -->
    <section id="patents" class="section py-5">
        <div class="container">
            <h2 class="section-title mb-4">Patents</h2>

            <!-- Patent Card 1 -->
            <div class="card publication-card mb-3">
                <div class="card-body">
                    <!-- Patent Title with Link -->
                    <h5 class="card-title">
                        <a href="https://register.dpma.de/DPMAregister/pat/register?AKZ=E231588492" target="_blank">
                            Method for finding the cause of detection failures of an artificial neural network
                        </a>
                    </h5>
                    <!-- Patent ID -->
                    <p class="card-text">
                        EP 4421682 A1
                    </p>
                    <!-- Authors -->
                    <p class="card-text-small">
                        <strong>Georgii Mikriukov</strong>, Christian Hellert, Erwin Kraft, Gesina Schwalbe.
                    </p>
                </div>
            </div>

            <!-- Add more patent cards as needed -->

        </div>
    </section>



    <!-- Contact Section -->
    <section id="contact" class="section bg-light py-5">
        <div class="container">
            <h2 class="section-title mb-4">Contact</h2>
            <div class="row">
                <div class="col-md-6">
                    <p><i class="fas fa-envelope me-2"></i> <a
                            href="mailto:mikriukov.georgii@gmail.com">mikriukov.georgii@gmail.com</a></p>
                    <p><i class="fab fa-linkedin me-2"></i> <a href="https://linkedin.com/in/georgii-mikriukov"
                            target="_blank">linkedin.com/in/georgii-mikriukov</a></p>
                    <p><i class="fab fa-github me-2"></i> <a href="https://github.com/comrados"
                            target="_blank">github.com/comrados</a></p>
                </div>
            </div>
        </div>
    </section>



    <!-- Footer -->
    <footer class="text-white py-4">
        <div class="container text-center">
            <p class="mb-0">&copy; 2025 Georgii Mikriukov. All Rights Reserved.</p>
        </div>
    </footer>



    <!-- Bootstrap Bundle JS -->
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
    <!-- Custom JS -->
    <script src="assets/js/scripts.js"></script>
</body>

</html>