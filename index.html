<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Georgii Mikriukov</title>
    <!-- Bootswatch Cosmo Theme (Bootstrap 5) -->
    <link href="https://cdn.jsdelivr.net/npm/bootswatch@5.3.0/dist/cosmo/bootstrap.min.css" rel="stylesheet" />
    <!-- Font Awesome Icons -->
    <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet" />
    <!-- Animate.css for animations -->
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/4.1.1/animate.min.css" />
    <!-- Custom CSS -->
    <link rel="stylesheet" href="assets/css/styles.css" />
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;600;700&display=swap" rel="stylesheet">
</head>



<body>
    <!-- Navigation Bar -->
    <nav class="navbar navbar-expand-lg navbar-dark bg-primary fixed-top">
        <div class="container">
            <a class="navbar-brand" href="#"><strong>Georgii Mikriukov</strong></a>
            <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarNav">
                <span class="navbar-toggler-icon"></span>
            </button>
            <div class="collapse navbar-collapse" id="navbarNav">
                <ul class="navbar-nav ms-auto">
                    <li class="nav-item"><a class="nav-link" href="#about">About</a></li>
                    <li class="nav-item"><a class="nav-link" href="#projects">Projects</a></li>
                    <li class="nav-item"><a class="nav-link" href="#publications">Publications</a></li>
                    <li class="nav-item"><a class="nav-link" href="#patents">Patents and Inventions</a></li>
                    <li class="nav-item"><a class="nav-link" href="#contact">Contact</a></li>
                </ul>
            </div>
        </div>
    </nav>



    <!-- Header / Hero Section -->
    <header class="hero-section text-white" style="padding-top:100px; padding-bottom:100px;">
        <div class="container">
            <div class="row align-items-center">
                <div class="col-md-8">
                    <h1 class="display-4"><strong>Georgii Mikriukov</strong></h1>
                    <p class="lead">
                        <strong>AI Research Scientist &amp; PhD Student</strong><br>
                        <small>Berlin, Germany</small>
                    </p>
                    <div class="social-links mt-3">
                        <a href="mailto:mikriukov.georgii@gmail.com" class="text-white me-3">
                            <i class="fas fa-envelope"></i>
                        </a>
                        <a href="https://github.com/comrados" class="text-white me-3" target="_blank">
                            <i class="fab fa-github"></i>
                        </a>
                        <a href="https://linkedin.com/in/georgii-mikriukov" class="text-white me-3" target="_blank">
                            <i class="fab fa-linkedin"></i>
                        </a>
                        <a href="https://scholar.google.com/citations?user=hCu7avEAAAAJ&hl=en" class="text-white me-3"
                            target="_blank">
                            <i class="fas fa-graduation-cap"></i>
                        </a>
                        <!-- ResearchGate Icon -->
                        <a href="https://www.researchgate.net/profile/Georgii-Mikriukov-2" class="text-white me-3"
                            target="_blank">
                            <i class="fab fa-researchgate"></i>
                        </a>
                    </div>
                    <a href="assets/cv/CV_Georgii_Mikriukov.pdf" download class="btn btn-cv btn-lg mt-3">
                        <i class="fas fa-download me-2"></i>Download CV
                    </a>
                </div>
                <div class="col-md-4 text-center">
                    <img src="assets/images/photo.jpg" alt="Georgii Mikriukov" class="img-fluid rounded-circle shadow"
                        style="max-width: 200px;">
                </div>
            </div>
        </div>
    </header>



    <!-- About Me Section -->
    <section id="about" class="section py-5">
        <div class="container">
            <h2 class="section-title mb-4">About Me</h2>
            <p>
                <strong>AI Research Scientist and PhD Student </strong> with experience in
                <strong>machine
                    learning and deep learning</strong>, specializing in <strong>computer vision, NLP, and eXplainable
                    AI</strong>. Extensive hands-on experience in developing and evaluating <strong>CNNs, ViTs, LLMs,
                    and multi-modal architectures</strong>. Proven expertise in <strong>model verification</strong> and
                <strong>large-scale multi-modal information retrieval</strong>, particularly in <strong>remote sensing
                    and autonomous driving</strong>.
            </p>
        </div>
    </section>



    <!-- Projects Section -->
    <section id="projects" class="section py-5">
        <div class="container">
            <h2 class="section-title mb-4">Selected Projects</h2>

            <!-- Scrollable Project Tiles -->
            <div class="projects-container">
                <div class="project-tile" data-project="1">
                    <h5><strong>Local Concept Embeddings</strong></h5>
                </div>
                <div class="project-tile" data-project="2">
                    <h5><strong>Concept-Based Adversarial Attack Analysis</strong></h5>
                </div>
                <div class="project-tile" data-project="3">
                    <h5><strong>Deep Unsupervised Contrastive Hashing</strong></h5>
                </div>
                <div class="project-tile" data-project="4">
                    <h5><strong>Cross-Modal Hashing with Noise Robustness</strong></h5>
                </div>
            </div>

            <!-- Project Details (Initially Hidden) -->
            <div class="project-details-container">
                <!-- Project 1 -->
                <div class="project-details" data-project="1">
                    <h4><strong>Local Concept Embeddings</strong></h4>
                    <p>
                        <strong>LoCEs (Local Concept Embeddings)</strong> provide a way to analyze how <strong>DNNs
                            represent object concepts</strong> in complex, real-world scenes. Unlike traditional global
                        approaches, LoCEs generate <strong>sample-specific embeddings</strong> that capture both the
                        target object and its surrounding <strong>context</strong> within a single, compact
                        representation.
                    </p>
                    <p>
                        This <strong>context-aware analysis</strong> helps uncover how models encode, separate, and
                        confuse visual concepts across diverse scenarios. LoCEs reveal meaningful patterns in feature
                        space, supporting model inspection, debugging, and evaluation.
                    </p>

                    <p><strong>Use cases include:</strong></p>
                    <ul>
                        <li><strong>Concept Understanding</strong> – Examine how models distinguish objects and their
                            contexts.</li>
                        <li><strong>Sub-Concept Discovery</strong> – Identify unlabeled variations within categories
                            (e.g., <strong>near</strong> vs. <strong>distant car</strong>).</li>
                        <li><strong>Concept Confusion Detection</strong> – Detect overlaps in representations of similar
                            categories (e.g., <strong>bus</strong> vs. <strong>truck</strong>).</li>
                        <li><strong>Outlier Detection</strong> – Find unusual or challenging examples in the data.</li>
                        <li><strong>Information Retrieval</strong> – Search for samples using LoCE-based similarity.
                        </li>
                        <li><strong>Model Comparison</strong> – Compare internal feature spaces across architectures or
                            training methods.</li>
                    </ul>

                    <div class="project-image-container text-center">
                        <figure>
                            <img src="assets/images/loce_example.png" alt="LoCEs Example"
                                class="img-fluid rounded shadow project-image-preview"
                                onclick="openModal('assets/images/loce_example.png')">
                            <figcaption class="mt-2 text-muted">LoCE optimization and generalization.</figcaption>
                        </figure>
                    </div>

                    <div class="project-image-container text-center">
                        <img src="assets/images/loce_separation.png" alt="LoCEs Concept Separation"
                            class="img-fluid rounded shadow project-image-preview"
                            onclick="openModal('assets/images/loce_separation.png')">
                        <figcaption class="mt-2 text-muted">Concept confusion across models revealed by LoCEs.
                        </figcaption>
                    </div>

                    <div class="text-center mt-3">
                        <a href="https://github.com/continental/local-concept-embeddings" target="_blank"
                            class="btn btn-github btn-lg">
                            <i class="fab fa-github me-2"></i> View on GitHub
                        </a>
                    </div>
                </div>
            </div>


            <!-- Project 2 -->
            <div class="project-details" data-project="2">
                <h4><strong>Concept-Based Adversarial Attack Analysis</strong></h4>
                <p>
                    <strong>Concept-Based Adversarial Analysis</strong> investigates how <strong>adversarial attacks
                        manipulate DNNs</strong>
                    at the <strong>concept level</strong>. This study reveals how
                    adversarial attacks <strong>distort, introduce, or remove concepts</strong> (i.e., latent
                    features) in a
                    model’s feature space.
                </p>

                <p>
                    Through <strong>concept discovery</strong> techniques, adversarial perturbations are decomposed
                    into components,
                    and their effects are analyzed across multiple DNN architectures and attack types.
                    It is revealed that adversarial attacks systematically distort concept representations, with
                    adversarial perturbations being <strong>linearly decomposable
                        into a small set of shared latent vectors</strong>. Analysis
                    demonstrates that <strong>attack components exploit target-specific directions</strong>.
                </p>

                <div class="project-image-container text-center">
                    <figure>
                        <img src="assets/images/aa_concept_change.png" alt="Concept changes under adversarial attacks"
                            class="img-fluid rounded shadow project-image-preview"
                            onclick="openModal('assets/images/aa_concept_change.png')">
                        <figcaption class="mt-2 text-muted">Concept distortions under adversarial attacks.
                        </figcaption>
                    </figure>
                </div>

                <div class="project-image-container text-center">
                    <img src="assets/images/aa_directions.png"
                        alt="Similarity of directions of concepts discovered in adversarial attacks targeting 'taxi' class."
                        class="img-fluid rounded shadow project-image-preview"
                        onclick="openModal('assets/images/aa_directions.png')">
                    <figcaption class="mt-2 text-muted">Similarity of directions of concepts discovered in
                        adversarial attacks targeting 'taxi' class.
                    </figcaption>
                </div>
                <div class="text-center mt-3">
                    <a href="https://link.springer.com/chapter/10.1007/978-3-031-63787-2_6" target="_blank"
                        class="btn btn-pdf btn-lg">
                        <i class="fas fa-file-pdf me-2"></i> Read paper
                    </a>
                </div>
            </div>




            <!-- Project 3 -->
            <div class="project-details" data-project="3">
                <h4><strong>Deep Unsupervised Contrastive Hashing</strong></h4>
                <p>
                    <strong>DUCH</strong> (Deep Unsupervised Contrastive Hashing) is an <strong>unsupervised
                        cross-modal retrieval method</strong> designed for efficient search and retrieval of
                    <strong>semantically related images and text</strong> in large-scale datasets.
                    Unlike traditional supervised retrieval methods that rely on extensive labeled data,
                    <strong>DUCH</strong> learns <strong>discriminative feature representations</strong> in an
                    unsupervised manner, making it scalable and adaptable for various multi-modal applications.
                </p>

                <h5>Key Features:</h5>
                <ul>
                    <li><strong>Cross-Modal Hashing:</strong> Generates <strong>compact binary hash codes</strong>
                        for <strong>fast and memory-efficient</strong> retrieval across different modalities (e.g.,
                        image-to-text, text-to-image).</li>
                    <li><strong>Unsupervised Contrastive Learning:</strong> Leverages <strong>contrastive
                            objectives</strong> to preserve semantic relationships, enabling effective retrieval
                        without labeled training data.</li>
                    <li><strong>Adversarial Learning:</strong> Enforces alignment between <strong>image and text
                            representations</strong> using an adversarial loss function, improving retrieval
                        consistency.</li>
                    <li><strong>Multi-Objective Optimization:</strong> Combines multiple loss functions to enhance
                        retrieval performance.
                    </li>
                </ul>

                <h5>Applications:</h5>
                <ul>
                    <li><strong>Multi-Modal Search:</strong> Enables <strong>image-to-text and text-to-image
                            retrieval</strong> in diverse domains, including visual recognition, media analysis, and
                        scientific research.</li>
                    <li><strong>Scalable Indexing:</strong> Compact hash codes enable <strong>efficient large-scale
                            data retrieval</strong> with minimal storage and computational cost.</li>
                    <li><strong>Semantic Representation Learning:</strong> Captures complex relationships between
                        different modalities, aiding in knowledge discovery and AI-driven content understanding.
                    </li>
                </ul>


                <div class="project-image-container text-center">
                    <img src="assets/images/duch_example.png" alt="DUCH Architecture"
                        class="img-fluid rounded shadow project-image-preview"
                        onclick="openModal('assets/images/duch_example.png')">
                    <figcaption class="mt-2 text-muted">DUCH Architecture.</figcaption>
                </div>

                <div class="text-center mt-3">
                    <a href="https://git.tu-berlin.de/rsim/duch" target="_blank" class="btn btn-github btn-lg">
                        <i class="fab fa-gitlab me-2"></i> View on GitLab
                    </a>
                </div>
            </div>

            <!-- Project 4 -->
            <div class="project-details" data-project="4">
                <h4><strong>Cross-Modal Hashing with Noise Robustness</strong></h4>
                <p>
                    <strong>CHNR</strong> (Cross-Modal Hashing with Noise Robustness) is an
                    <strong>unsupervised</strong> technique designed for
                    <strong>retrieving images based on textual descriptions</strong>, even in the presence of noisy
                    image-text correspondences.
                    It extends <a href="#" class="duch-link"><strong>DUCH (Deep Unsupervised Contrastive
                            Hashing)</strong></a> by introducing a <strong>noise detection module</strong> that
                    mitigates errors in training data.
                </p>

                <ul>
                    <li><strong>Unsupervised Learning</strong> – Learns robust cross-modal representations without
                        labeled data, making it adaptable to large-scale datasets.</li>
                    <li><strong>Robust Feature Extraction</strong> – Extracts deep representations of both images
                        and texts, ensuring effective cross-modal learning.</li>
                    <li><strong>Noise Detection Module</strong> – Identifies and reduces the impact of incorrectly
                        paired image-text data.</li>
                    <li><strong>Efficient Hashing Mechanism</strong> – Generates binary hash codes for scalable
                        retrieval with minimal storage requirements.</li>
                </ul>


                <div class="project-image-container text-center">
                    <img src="assets/images/chnr_example.png" alt="CHNR Architecture"
                        class="img-fluid rounded shadow project-image-preview"
                        onclick="openModal('assets/images/chnr_example.png')">
                    <figcaption class="mt-2 text-muted">CHNR Architecture.</figcaption>
                </div>

                <div class="text-center mt-3">
                    <a href="https://git.tu-berlin.de/rsim/chnr" target="_blank" class="btn btn-github btn-lg">
                        <i class="fab fa-gitlab me-2"></i> View on GitLab
                    </a>
                </div>
            </div>

        </div>

        <!-- Full-Screen Image Modal -->
        <div id="imageModal" class="modal" onclick="closeModal()">
            <span class="close">&times;</span>
            <img class="modal-content" id="fullImage">
        </div>
        </div>
    </section>


    <!-- Publications Section -->
    <section id="publications" class="section py-5">
        <div class="container">
            <h2 class="section-title mb-4">Publications</h2>

            <div>
                <h4 class="publication-year">2025</h4>
            </div>

            <div class="card publication-card mb-3">
                <div class="card-body">
                    <h5 class="card-title"><strong>On Background Bias of Post-Hoc Concept Embeddings in Computer Vision
                            DNNs</strong></h5>
                    <p class="card-text">Explainable Artificial Intelligence, xAI 2025.</p>
                    <p class="card-text-small">Gesina Schwalbe, <strong>Georgii Mikriukov</strong>, Stavros Gerolymatos,
                        Edgar Heinert, Annika Mütze, Mert Keser, Alois Knoll, Matthias Rottmann.</p>
                    <div class="d-flex d-flex gap-2 mt-3">
                        <a href="https://arxiv.org/pdf/2504.08602" target="_blank" class="btn btn-pdf">
                            <i class="fas fa-file-pdf me-2"></i> PDF
                        </a>
                        <a href="https://github.com/gesina/bg_randomized_loce" target="_blank" class="btn btn-github">
                            <i class="fab fa-github me-2"></i> GitHub
                        </a>
                    </div>
                </div>
            </div>

            <div class="card publication-card mb-3">
                <div class="card-body">
                    <h5 class="card-title"><strong>Local Concept Embeddings for Analysis of Concept Distributions in
                            Vision DNN Feature Spaces</strong></h5>
                    <p class="card-text">International Journal of Computer Vision (IJCV), May, 2025.</p>
                    <p class="card-text-small"><strong>Georgii Mikriukov</strong>, Gesina Schwalbe, Korinna Bade.</p>
                    <div class="d-flex d-flex gap-2 mt-3">
                        <a href="https://link.springer.com/article/10.1007/s11263-025-02446-y" target="_blank"
                            class="btn btn-pdf">
                            <i class="fas fa-file-pdf me-2"></i> PDF
                        </a>
                        <a href="https://github.com/continental/local-concept-embeddings" target="_blank"
                            class="btn btn-github">
                            <i class="fab fa-github me-2"></i> GitHub
                        </a>
                    </div>
                </div>
            </div>

            <div>
                <h4 class="publication-year">2024</h4>
            </div>


            <div class="card publication-card mb-3">
                <div class="card-body">
                    <h5 class="card-title"><strong>Concept-Based Explanations in Computer Vision: Where Are We
                            Going?</strong></h5>
                    <p class="card-text">Explainable Computer Vision @ European Conference on Computer Vision, ECCV
                        2024.</p>
                    <p class="card-text-small">Jae Hee Lee, <strong>Georgii Mikriukov</strong>, Gesina Schwalbe, Stefan
                        Wermter, Diedrich Wolter.</p>
                    <div class="mt-3">
                        <a href="https://excv-workshop.github.io/publication/concept-based-explanations-in-computer-vision-where-are-we-and-where-could-we-go/paper.pdf"
                            target="_blank" class="btn btn-pdf">
                            <i class="fas fa-file-pdf me-2"></i> PDF
                        </a>
                    </div>
                </div>
            </div>


            <div class="card publication-card mb-3">
                <div class="card-body">
                    <h5 class="card-title"><strong>Locally Testing Model Detections for Semantic Global
                            Concepts</strong></h5>
                    <p class="card-text">Explainable Artificial Intelligence, xAI 2024.</p>
                    <p class="card-text-small">Franz Motzkus, <strong>Georgii Mikriukov</strong>, Christian Hellert, Ute
                        Schmid.</p>
                    <div class="mt-3">
                        <a href="https://link.springer.com/chapter/10.1007/978-3-031-63787-2_8" target="_blank"
                            class="btn btn-pdf">
                            <i class="fas fa-file-pdf me-2"></i> PDF
                        </a>
                    </div>
                </div>
            </div>


            <div class="card publication-card mb-3">
                <div class="card-body">
                    <h5 class="card-title"><strong>Unveiling the Anatomy of Adversarial Attacks: Concept-Based XAI
                            Dissection of CNNs</strong></h5>
                    <p class="card-text">Explainable Artificial Intelligence, xAI 2024.</p>
                    <p class="card-text-small"><strong>Georgii Mikriukov</strong>, Gesina Schwalbe, Franz Motzkus,
                        Korinna Bade.</p>
                    <div class="mt-3">
                        <a href="https://link.springer.com/chapter/10.1007/978-3-031-63787-2_6" target="_blank"
                            class="btn btn-pdf">
                            <i class="fas fa-file-pdf me-2"></i> PDF
                        </a>
                    </div>
                </div>
            </div>

            <div>
                <h4 class="publication-year">2023</h4>
            </div>

            <div class="card publication-card mb-3">
                <div class="card-body">
                    <h5 class="card-title"><strong>Revealing Similar Semantics Inside CNNs: An Interpretable
                            Concept-Based
                            Comparison of Feature Spaces</strong>
                    </h5>
                    <p class="card-text">
                        Machine Learning and Principles and Practice of Knowledge Discovery in Databases, ECML PKDD 2023
                    </p>
                    <p class="card-text-small"><strong>Georgii Mikriukov</strong>, Gesina Schwalbe, Christian Hellert,
                        Korinna Bade.</p>
                    <div class="mt-3">
                        <a href="https://link.springer.com/chapter/10.1007/978-3-031-74630-7_1" target="_blank"
                            class="btn btn-pdf">
                            <i class="fas fa-file-pdf me-2"></i> PDF
                        </a>
                    </div>
                </div>
            </div>

            <div class="card publication-card mb-3">
                <div class="card-body">
                    <h5 class="card-title">
                        <strong>Evaluating the Stability of Semantic Concept Representations in CNNs</strong>
                    </h5>
                    <p class="card-text">
                        Explainable Artificial Intelligence, xAI 2023.
                    </p>
                    <p class="card-text-small">
                        <strong>Georgii Mikriukov</strong>, Gesina Schwalbe, Christian Hellert, Korinna Bade.
                    </p>
                    <p class="card-text-small award-highlight">
                        Best industry paper award:
                        <a href="https://xaiworldconference.com/2023/awards/" target="_blank">
                            Awards – The World Conference on eXplainable Artificial Intelligence
                        </a>
                    </p>
                    <div class="mt-3">
                        <a href="https://link.springer.com/chapter/10.1007/978-3-031-44067-0_26" target="_blank"
                            class="btn btn-pdf">
                            <i class="fas fa-file-pdf me-2"></i> PDF
                        </a>
                    </div>
                </div>
            </div>

            <div>
                <h4 class="publication-year">2022</h4>
            </div>


            <div class="card publication-card mb-3">
                <div class="card-body">
                    <h5 class="card-title"><strong>An Unsupervised Cross-Modal Hashing Method Robust to Noisy Training
                            Image-Text Correspondences</strong></h5>
                    <p class="card-text">IEEE International Conference on Image Processing, ICIP 2022.</p>
                    <p class="card-text-small"><strong>Georgii Mikriukov</strong>, Mahdyar Ravanbakhsh, Begüm Demir.</p>
                    <div class="d-flex d-flex gap-2 mt-3">
                        <a href="https://ieeexplore.ieee.org/document/9897500" target="_blank" class="btn btn-pdf">
                            <i class="fas fa-file-pdf me-2"></i> PDF
                        </a>
                        <a href="https://git.tu-berlin.de/rsim/duch" target="_blank" class="btn btn-github">
                            <i class="fab fa-gitlab me-2"></i> GitLab
                        </a>
                    </div>
                </div>
            </div>


            <div class="card publication-card mb-3">
                <div class="card-body">
                    <h5 class="card-title"><strong>Unsupervised Contrastive Hashing for Cross-Modal Retrieval in Remote
                            Sensing</strong></h5>
                    <p class="card-text">IEEE International Conference on Acoustics, Speech and Signal Processing,
                        ICASSP 2022.</p>
                    <p class="card-text-small"><strong>Georgii Mikriukov</strong>, Mahdyar Ravanbakhsh, Begüm Demir.</p>
                    <div class="d-flex d-flex gap-2 mt-3">
                        <a href="https://ieeexplore.ieee.org/document/9746251" target="_blank" class="btn btn-pdf">
                            <i class="fas fa-file-pdf me-2"></i> PDF
                        </a>
                        <a href="https://git.tu-berlin.de/rsim/chnr" target="_blank" class="btn btn-github">
                            <i class="fab fa-gitlab me-2"></i> GitLab
                        </a>
                    </div>
                </div>
            </div>

        </div>
    </section>




    <!-- Patents Section -->
    <section id="patents" class="section py-5">
        <div class="container">
            <h2 class="section-title mb-4">Patents and Inventions</h2>

            <!-- Patent Card 1 -->
            <div class="card publication-card mb-3">
                <div class="card-body">
                    <!-- Patent Title -->
                    <h5 class="card-title"><strong>Method for Finding the Cause of Detection Failures of an Artificial
                            Neural Network</strong></h5>

                    <!-- Patent ID -->
                    <p class="card-text">EP 4421682 A1</p>

                    <!-- Authors -->
                    <p class="card-text-small">
                        <strong>Georgii Mikriukov</strong>, Christian Hellert, Erwin Kraft, Gesina Schwalbe.
                    </p>

                    <!-- Patent Button -->
                    <div class="mt-3">
                        <a href="https://patents.google.com/patent/EP4421682A1/" target="_blank" class="btn btn-patent">
                            <i class="fas fa-file-alt me-2"></i> View Patent
                        </a>
                    </div>
                </div>
            </div>

            <!-- Add more patent cards as needed -->

        </div>
    </section>



    <!-- Contact Section -->
    <section id="contact" class="section bg-light py-5">
        <div class="container">
            <h2 class="section-title mb-4">Contact</h2>
            <div class="row">
                <div class="col-md-6">
                    <p><i class="fas fa-envelope me-2"></i> <a
                            href="mailto:mikriukov.georgii@gmail.com">mikriukov.georgii@gmail.com</a></p>
                    <p><i class="fab fa-linkedin me-2"></i> <a href="https://linkedin.com/in/georgii-mikriukov"
                            target="_blank">linkedin.com/in/georgii-mikriukov</a></p>
                    <p><i class="fab fa-github me-2"></i> <a href="https://github.com/comrados"
                            target="_blank">github.com/comrados</a></p>
                </div>
            </div>
        </div>
    </section>



    <!-- Footer -->
    <footer class="text-white py-4">
        <div class="container text-center">
            <p class="mb-0">&copy; 2025 Georgii Mikriukov. All Rights Reserved.</p>
        </div>
    </footer>



    <!-- Bootstrap Bundle JS -->
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
    <!-- Custom JS -->
    <script src="assets/js/scripts.js"></script>
</body>

</html>